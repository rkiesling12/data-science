{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "\n",
    "# removing everything except alphabets`\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# removing short words\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# make all text lowercase\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# tokenization\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "\n",
    "# remove stop-words\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "# de-tokenization\n",
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "news_df['clean_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "max_features= 1000, # keep top 1000 terms \n",
    "max_df = 0.5, \n",
    "smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "\n",
    "# Getting the word list.\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# Number of topics.\n",
    "ntopics=20\n",
    "\n",
    "# Linking words to topics\n",
    "def word_topic(tfidf,solution, wordlist):\n",
    "    \n",
    "    # Loading scores for each word on each topic/component.\n",
    "    words_by_topic=tfidf.T * solution\n",
    "\n",
    "    # Linking the loadings to the words in an easy-to-read way.\n",
    "    components=pd.DataFrame(words_by_topic,index=wordlist)\n",
    "    \n",
    "    return components\n",
    "\n",
    "# Extracts the top N words and their loadings for each topic.\n",
    "def top_words(components, n_top_words):\n",
    "    n_topics = range(components.shape[1])\n",
    "    index= np.repeat(n_topics, n_top_words, axis=0)\n",
    "    topwords=pd.Series(index=index)\n",
    "    for column in range(components.shape[1]):\n",
    "        # Sort the column so that highest loadings are at the top.\n",
    "        sortedwords=components.iloc[:,column].sort_values(ascending=False)\n",
    "        # Choose the N highest loadings.\n",
    "        chosen=sortedwords[:n_top_words]\n",
    "        # Combine loading and index into a string.\n",
    "        chosenlist=chosen.index +\" \"+round(chosen,2).map(str) \n",
    "        topwords.loc[column]=[x for x in chosenlist]\n",
    "    return(topwords)\n",
    "\n",
    "# Number of words to look at for each topic.\n",
    "n_top_words = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "svd= TruncatedSVD(ntopics)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "news_lsa = lsa.fit_transform(X)\n",
    "\n",
    "components_lsa = word_topic(X, news_lsa, terms)\n",
    "\n",
    "topwords=pd.DataFrame()\n",
    "topwords['LSA']=top_words(components_lsa, n_top_words)                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "lda = LDA(n_components=ntopics, \n",
    "          doc_topic_prior=None, # Prior = 1/n_documents\n",
    "          topic_word_prior=1/ntopics,\n",
    "          learning_decay=0.7, # Convergence rate.\n",
    "          learning_offset=10.0, # Causes earlier iterations to have less influence on the learning\n",
    "          max_iter=10, # when to stop even if the model is not converging (to prevent running forever)\n",
    "          evaluate_every=-1, # Do not evaluate perplexity, as it slows training time.\n",
    "          mean_change_tol=0.001, # Stop updating the document topic distribution in the E-step when mean change is < tol\n",
    "          max_doc_update_iter=100, # When to stop updating the document topic distribution in the E-step even if tol is not reached\n",
    "          n_jobs=-1, # Use all available CPUs to speed up processing time.\n",
    "          verbose=0, # amount of output to give while iterating\n",
    "          random_state=0\n",
    "         )\n",
    "\n",
    "news_lda = lda.fit_transform(X) \n",
    "\n",
    "components_lda = word_topic(X, news_lda, terms)\n",
    "\n",
    "topwords['LDA']=top_words(components_lda, n_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNMF\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(alpha=0.0, \n",
    "          init='nndsvdar', # how starting value are calculated\n",
    "          l1_ratio=0.0, # Sets whether regularization is L2 (0), L1 (1), or a combination (values between 0 and 1)\n",
    "          max_iter=200, # when to stop even if the model is not converging (to prevent running forever)\n",
    "          n_components=ntopics, \n",
    "          random_state=0, \n",
    "          solver='cd', # Use Coordinate Descent to solve\n",
    "          tol=0.0001, # model will stop if tfidf-WH <= tol\n",
    "          verbose=0 # amount of output to give while iterating\n",
    "         )\n",
    "news_nmf = nmf.fit_transform(X) \n",
    "\n",
    "components_nmf = word_topic(X, news_nmf, terms)\n",
    "\n",
    "topwords['NNMF']=top_words(components_nmf, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "             LSA              LDA         NNMF\n",
      "0    like 164.83       sale 18.32  really 3.82\n",
      "0     know 151.4   condition 15.6    like 3.77\n",
      "0  people 144.61      offer 12.99    make 3.58\n",
      "0    think 134.3      price 10.94    know 3.33\n",
      "0    good 114.38  excellent 10.12    want 3.32\n",
      "0    time 111.31       email 9.68  people 3.19\n",
      "0      make 95.0        good 9.47   think 3.13\n",
      "0     want 90.11      asking 9.28    sure 3.06\n",
      "0    right 86.04        best 8.56     good 2.8\n",
      "0   really 83.42        sell 6.91    going 2.8\n",
      "Topic 1:\n",
      "              LSA          LDA              NNMF\n",
      "1    thanks 75.23   like 11.55      thanks 13.63\n",
      "1   windows 58.76    bike 8.68          mail 5.3\n",
      "1      card 36.01    good 7.77      advance 4.67\n",
      "1      mail 35.31    know 7.51         know 3.51\n",
      "1     drive 32.96    time 7.17      looking 3.09\n",
      "1      file 30.79    dave 7.08         help 2.72\n",
      "1  software 30.29  thanks 7.03         info 2.69\n",
      "1   advance 29.39    mike 5.82  information 2.38\n",
      "1   program 27.62  sounds 5.15        email 2.34\n",
      "1   looking 26.53    mail 5.01         like 2.21\n",
      "Topic 2:\n",
      "             LSA               LDA         NNMF\n",
      "2     game 57.18      people 70.53   game 10.74\n",
      "2     team 52.98  government 63.87    team 9.48\n",
      "2     year 48.38        like 38.27    year 7.86\n",
      "2    games 43.39       think 34.34   games 6.91\n",
      "2   season 31.91        know 32.91  season 5.15\n",
      "2  players 28.91        time 32.12  players 4.6\n",
      "2     play 27.64      public 31.75    play 4.48\n",
      "2     good 25.46       state 30.87  hockey 4.11\n",
      "2    hockey 24.6       right 30.78  league 3.51\n",
      "2   league 21.44  encryption 29.74   think 3.19\n",
      "Topic 3:\n",
      "                LSA            LDA             NNMF\n",
      "3        drive 63.7     chip 11.28      drive 20.27\n",
      "3        scsi 28.33   number 10.08        scsi 7.39\n",
      "3        hard 23.91      know 7.71        hard 5.74\n",
      "3        disk 22.32      like 7.31        disk 5.29\n",
      "3      drives 19.02   clipper 6.95      drives 5.23\n",
      "3        card 17.88     think 6.35  controller 3.59\n",
      "3     problem 17.32     phone 6.08      floppy 3.45\n",
      "3        work 14.66      keys 5.83      thanks 2.12\n",
      "3  controller 14.39  computer 5.78    computer 2.11\n",
      "3       speed 13.94      used 5.05       power 2.09\n",
      "Topic 4:\n",
      "             LSA            LDA           NNMF\n",
      "4  windows 48.61  article 12.37  windows 17.17\n",
      "4     file 32.02     like 10.48     files 3.03\n",
      "4   window 26.17      know 9.97   version 2.87\n",
      "4  program 22.79    thanks 8.83      file 2.77\n",
      "4    files 22.16      says 6.49    thanks 2.62\n",
      "4    using 22.12    source 6.06   drivers 2.55\n",
      "4  version 15.62      mail 5.72      using 2.4\n",
      "4  running 14.53      news 5.39   program 2.36\n",
      "4  problem 14.28  question 5.32  software 2.28\n",
      "4     team 12.47      line 5.24    running 2.2\n",
      "Topic 5:\n",
      "                 LSA             LDA             NNMF\n",
      "5  information 24.78     thanks 51.7        chip 7.81\n",
      "5   government 24.29   windows 49.61     clipper 5.24\n",
      "5         data 23.15     files 33.39  encryption 5.18\n",
      "5         mail 22.58      know 32.73  government 5.05\n",
      "5       number 21.45      help 29.33        keys 3.74\n",
      "5         used 21.15   advance 28.05      number 3.16\n",
      "5        phone 21.11  software 25.41       phone 3.07\n",
      "5          chip 20.0      need 24.96         data 3.0\n",
      "5        email 19.38      mail 24.05      public 2.82\n",
      "5         send 18.58   program 22.98      escrow 2.49\n",
      "Topic 6:\n",
      "            LSA            LDA         NNMF\n",
      "6     like 93.0   israel 23.72   like 21.55\n",
      "6    bike 19.39     jews 14.93    know 4.26\n",
      "6    know 16.12  israeli 13.92   think 3.64\n",
      "6    sure 15.59    jewish 12.8  people 3.51\n",
      "6    chip 14.23      arab 9.74  sounds 3.09\n",
      "6  sounds 14.15      like 8.23    good 2.93\n",
      "6    look 13.93      know 7.24    time 2.73\n",
      "6   looks 13.01     think 7.07    look 2.65\n",
      "6     make 9.85     peace 6.59    make 2.41\n",
      "6     work 9.48      said 6.59   looks 2.35\n",
      "Topic 7:\n",
      "               LSA          LDA          NNMF\n",
      "7       card 44.05    soon 9.51    card 12.67\n",
      "7       sale 27.22   blood 7.44    video 6.15\n",
      "7      video 23.37    yeah 7.37  monitor 3.77\n",
      "7       price 19.9   right 6.03    cards 3.24\n",
      "7      offer 19.32    like 5.99  drivers 3.24\n",
      "7    monitor 18.08   think 5.43   driver 2.63\n",
      "7   shipping 16.57  course 5.36  windows 2.44\n",
      "7  condition 15.02    know 5.34    color 2.19\n",
      "7      jesus 14.58  doctor 5.23     know 2.11\n",
      "7       best 12.76  people 4.73   memory 2.01\n",
      "Topic 8:\n",
      "                LSA            LDA          NNMF\n",
      "8        know 54.18  shipping 17.2    know 17.69\n",
      "8        card 31.95  deleted 11.35     like 3.55\n",
      "8  government 21.21      sale 9.96   thanks 3.33\n",
      "8        chip 19.98      used 9.04   people 2.84\n",
      "8      people 17.24     stuff 8.82     think 2.4\n",
      "8       video 15.49      like 8.67      want 2.2\n",
      "8       right 13.09      offer 8.1  anybody 2.18\n",
      "8      driver 13.02      works 8.1     good 2.16\n",
      "8     drivers 12.51      know 6.62     need 2.05\n",
      "8      clipper 12.5  included 6.37     time 1.96\n",
      "Topic 9:\n",
      "              LSA           LDA             NNMF\n",
      "9      good 51.39     like 9.46     people 14.53\n",
      "9      know 31.07    space 7.26        think 3.2\n",
      "9      time 23.77    think 6.64        like 2.69\n",
      "9      used 13.64     know 6.26        know 2.57\n",
      "9      bike 12.67  station 5.73  government 2.53\n",
      "9   problem 12.12     time 5.66       right 1.91\n",
      "9  question 11.45      want 5.6        time 1.86\n",
      "9      long 11.09     good 5.59        make 1.77\n",
      "9     jesus 10.32     year 5.57        good 1.76\n",
      "9    believe 9.97   engine 5.32        want 1.61\n",
      "Topic 10:\n",
      "                LSA             LDA         NNMF\n",
      "10      think 85.31       file 9.42  think 18.87\n",
      "10       good 21.21       list 8.32  people 4.14\n",
      "10      thanks 12.3     thanks 8.11    like 3.56\n",
      "10       need 11.13        know 7.5    know 2.86\n",
      "10       chip 10.01     called 7.25    good 2.64\n",
      "10  encryption 7.96  directory 6.73    time 2.23\n",
      "10     clipper 7.75    looking 6.12  really 2.13\n",
      "10     looking 7.57       like 5.82    year 2.06\n",
      "10        keys 5.68       post 5.77    make 1.95\n",
      "10       phone 5.68    program 5.31   right 1.83\n",
      "Topic 11:\n",
      "                LSA                LDA          NNMF\n",
      "11       know 38.76        space 22.99  problem 9.73\n",
      "11      jesus 21.27      address 21.98   window 7.53\n",
      "11       data 15.44         mail 18.62    using 4.92\n",
      "11    believe 13.23       thanks 17.77   program 3.0\n",
      "11      bible 12.82  information 16.77  windows 2.87\n",
      "11       chip 12.32         send 15.67     like 2.76\n",
      "11       think 12.2        email 14.03    server 2.7\n",
      "11  available 11.56         nasa 13.57   screen 2.62\n",
      "11       list 11.15     internet 12.98     work 2.51\n",
      "11      space 10.96         info 12.86     know 2.49\n",
      "Topic 12:\n",
      "               LSA           LDA          NNMF\n",
      "12      good 38.88   drive 27.35    good 13.81\n",
      "12    people 32.56     scsi 19.7      like 2.5\n",
      "12      know 30.13  drives 13.75     think 2.3\n",
      "12    windows 23.7     know 8.06     know 2.22\n",
      "12      sale 18.51  problem 7.95     year 2.11\n",
      "12     price 15.25     like 7.85   people 1.94\n",
      "12      file 13.67   thanks 7.61     time 1.94\n",
      "12      year 13.48    think 6.43    thing 1.62\n",
      "12  condition 12.0    speed 6.02   better 1.54\n",
      "12      bike 11.42     hard 5.93  looking 1.49\n",
      "Topic 13:\n",
      "             LSA           LDA          NNMF\n",
      "13   space 40.05    bike 36.04   space 11.86\n",
      "13   think 30.11    like 32.67     nasa 4.57\n",
      "13    know 20.02    good 29.47     data 2.37\n",
      "13    nasa 16.52    know 29.38  program 2.35\n",
      "13   years 13.37    cars 26.52  shuttle 2.32\n",
      "13     year 12.9   think 25.59    launch 2.2\n",
      "13    said 10.83    time 22.72     like 1.93\n",
      "13   problem 9.7    right 21.7    orbit 1.87\n",
      "13  article 9.24  engine 20.66    earth 1.84\n",
      "13     post 8.74   thing 20.23  station 1.74\n",
      "Topic 14:\n",
      "             LSA                LDA            NNMF\n",
      "14   space 26.58       window 45.04        sale 7.1\n",
      "14    good 24.27        using 27.87      offer 4.88\n",
      "14     card 19.4         file 26.91      price 4.38\n",
      "14    time 18.08      windows 26.12      email 4.14\n",
      "14  thanks 13.34      problem 24.31  condition 4.03\n",
      "14     nasa 9.86       program 22.3   shipping 3.83\n",
      "14    heard 9.79       screen 21.84  interested 3.3\n",
      "14      data 9.7         like 21.73        best 3.2\n",
      "14    video 9.54  application 20.59        mail 3.1\n",
      "14     like 8.84         motif 20.5     asking 2.98\n",
      "Topic 15:\n",
      "              LSA             LDA            NNMF\n",
      "15   people 47.72    people 78.47    israel 10.42\n",
      "15     time 32.86     think 58.57    israeli 5.71\n",
      "15  problem 16.06     jesus 56.59       jews 5.04\n",
      "15     need 10.15   believe 48.14     jewish 3.21\n",
      "15   program 9.17      know 44.15       arab 3.15\n",
      "15      game 8.77      like 43.23        said 3.1\n",
      "15      used 8.09     bible 40.44      people 3.0\n",
      "15    memory 8.03      life 38.95   armenian 2.85\n",
      "15    screen 7.77  christian 37.6      state 2.83\n",
      "15      apple 7.7       time 37.1  armenians 2.56\n",
      "Topic 16:\n",
      "              LSA            LDA         NNMF\n",
      "16     time 38.49     card 45.21   right 12.8\n",
      "16     bike 27.84  drivers 24.92    bike 4.72\n",
      "16  windows 23.54    video 22.91    left 3.07\n",
      "16      need 21.4   driver 21.67  people 2.71\n",
      "16   really 15.18  windows 18.93     like 2.6\n",
      "16    going 14.28    cards 18.11    know 2.39\n",
      "16     said 10.31   version 15.3   think 2.17\n",
      "16     right 9.74   memory 15.26    time 1.76\n",
      "16      want 7.85  printer 14.78    good 1.65\n",
      "16      mail 7.46     know 14.17   going 1.61\n",
      "Topic 17:\n",
      "              LSA            LDA         NNMF\n",
      "17     time 29.99     game 62.09   time 15.81\n",
      "17  problem 27.45     team 60.29    long 3.67\n",
      "17     mail 22.33      year 51.2    like 3.13\n",
      "17     file 17.94    games 42.37  people 2.73\n",
      "17     list 12.97    season 37.3    know 2.73\n",
      "17  address 12.02  players 36.52   think 2.62\n",
      "17     year 10.29     play 35.13    good 2.57\n",
      "17      send 9.47    think 32.39    years 2.5\n",
      "17     think 9.37   hockey 29.93    year 1.68\n",
      "17       said 9.3   league 28.77  really 1.65\n",
      "Topic 18:\n",
      "              LSA              LDA             NNMF\n",
      "18     mail 30.97      drive 28.37       jesus 7.66\n",
      "18     need 25.65        disk 20.9     believe 5.16\n",
      "18   really 24.36      mouse 14.72       bible 4.15\n",
      "18     year 21.13       port 13.95      people 3.86\n",
      "18  problem 20.94  controller 13.9   christian 3.81\n",
      "18    right 20.43     floppy 13.71      christ 3.44\n",
      "18  address 15.72      apple 13.45  christians 3.38\n",
      "18   memory 14.56      modem 13.36      church 3.16\n",
      "18     send 11.65       hard 13.16       faith 2.98\n",
      "18    going 11.25    monitor 11.76        think 2.8\n",
      "Topic 19:\n",
      "              LSA           LDA            NNMF\n",
      "19     file 35.23     know 7.61      file 14.46\n",
      "19    right 24.14  posting 7.27      files 7.91\n",
      "19      year 18.7   ground 7.03    windows 4.18\n",
      "19  article 14.82  thought 7.01    program 3.72\n",
      "19  believe 14.16     like 6.44  directory 2.87\n",
      "19     card 12.71  michael 6.41      format 2.5\n",
      "19     make 12.68    think 5.66  available 2.25\n",
      "19     need 11.76    going 5.44       know 2.21\n",
      "19    files 11.67     good 5.37      image 2.13\n",
      "19       bike 9.9     sure 5.19   graphics 2.05\n"
     ]
    }
   ],
   "source": [
    "for topic in range(ntopics):\n",
    "    print('Topic {}:'.format(topic))\n",
    "    print(topwords.loc[topic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA 2 is rec.sport.hockey, LSA 13 is sci.space. The other topics are either indistiguishable from others in a similar category or indistinguishable overall. LDA 0 is misc.forsale, LDA 6 is talk.religion.misc, LDA 7 is sci.med, LDA 9 is sci.space, LDA 15 is soc.religion.christian, LDA 17 is rec.sport.hockey. Overall, the topics were more distiguishable in LDA than in LSA. NNMF 2 is rec.sport.hockey, NNMF 13 is sci.space, NNMF 14 is misc.forsale, NNMF 15 is talk.religion.misc, and NNMF 18 is soc.religion.christian. Some of the groups in NNMF were particularly unclear. Overall, I felt that LDA did the best job of creating topics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
